{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6837c96c",
   "metadata": {},
   "source": [
    "# ðŸŒŠ Tsunami Prediction Model Training - Google Colab\n",
    "\n",
    "This notebook trains the CNN-LSTM tsunami prediction model using Google Colab's free GPU resources.\n",
    "\n",
    "**Runtime:** GPU (recommended) or TPU\n",
    "\n",
    "**Steps:**\n",
    "1. Install dependencies\n",
    "2. Clone repository or upload files\n",
    "3. Prepare training data\n",
    "4. Train the model\n",
    "5. Download trained model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146f4410",
   "metadata": {},
   "source": [
    "## ðŸ“‹ Step 1: Check GPU/TPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def936e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import sys\n",
    "\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"\\nGPU Available:\", tf.config.list_physical_devices('GPU'))\n",
    "print(\"TPU Available:\", tf.config.list_physical_devices('TPU'))\n",
    "\n",
    "# Enable mixed precision for faster training\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"\\nâœ“ GPU detected - enabling mixed precision training\")\n",
    "    from tensorflow.keras import mixed_precision\n",
    "    policy = mixed_precision.Policy('mixed_float16')\n",
    "    mixed_precision.set_global_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ea2c8f",
   "metadata": {},
   "source": [
    "## ðŸ’¾ Step 2: Mount Google Drive (Optional)\n",
    "\n",
    "Mount Google Drive to save models and data persistently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66794c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create project directory in Drive\n",
    "project_dir = '/content/drive/MyDrive/tsunami_warning_system'\n",
    "os.makedirs(project_dir, exist_ok=True)\n",
    "os.makedirs(f'{project_dir}/models', exist_ok=True)\n",
    "os.makedirs(f'{project_dir}/data', exist_ok=True)\n",
    "\n",
    "print(f\"âœ“ Project directory: {project_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf118538",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Step 3: Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924ebf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/vsiva763-git/India-specific-tsunami-early-warning-system.git\n",
    "\n",
    "# Change to project directory\n",
    "%cd India-specific-tsunami-early-warning-system\n",
    "\n",
    "# List files\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2d578b",
   "metadata": {},
   "source": [
    "## ðŸ”§ Step 4: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c864e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q tensorflow==2.18.0\n",
    "!pip install -q pandas numpy scipy scikit-learn\n",
    "!pip install -q matplotlib seaborn plotly\n",
    "!pip install -q pyyaml joblib loguru\n",
    "!pip install -q xarray netCDF4\n",
    "\n",
    "print(\"âœ“ All dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409101e2",
   "metadata": {},
   "source": [
    "## ðŸ“Š Step 5: Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f286c590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q tensorflow==2.18.0\n",
    "!pip install -q pandas numpy scipy scikit-learn\n",
    "!pip install -q matplotlib seaborn plotly\n",
    "!pip install -q pyyaml joblib loguru\n",
    "!pip install -q xarray netCDF4\n",
    "\n",
    "print(\"âœ“ All dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d23565",
   "metadata": {},
   "source": [
    "## âš™ï¸ Step 6: Configure Model\n",
    "\n",
    "You can adjust training parameters here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c7466c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.config_loader import load_config\n",
    "\n",
    "# Load configuration\n",
    "config = load_config('config/config.yaml')\n",
    "\n",
    "# Override training parameters for faster training in Colab\n",
    "config['model']['training']['epochs'] = 50  # Adjust as needed\n",
    "config['model']['training']['batch_size'] = 64  # Larger batch for GPU\n",
    "config['model']['training']['learning_rate'] = 0.001\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  Epochs: {config['model']['training']['epochs']}\")\n",
    "print(f\"  Batch Size: {config['model']['training']['batch_size']}\")\n",
    "print(f\"  Learning Rate: {config['model']['training']['learning_rate']}\")\n",
    "print(f\"  Early Stopping Patience: {config['model']['training']['early_stopping_patience']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c07c4dd",
   "metadata": {},
   "source": [
    "## ðŸ—ï¸ Step 7: Build Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38c6e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import TsunamiPredictionModel, DataPreprocessor\n",
    "\n",
    "# Initialize model and preprocessor\n",
    "print(\"Initializing model...\")\n",
    "model = TsunamiPredictionModel(config)\n",
    "preprocessor = DataPreprocessor(config)\n",
    "\n",
    "# Build model architecture\n",
    "print(\"\\nBuilding CNN-LSTM architecture...\")\n",
    "model.build_model(\n",
    "    earthquake_shape=(10, 4),   # 10 earthquakes, 4 features\n",
    "    ocean_shape=(5, 3),         # 5 locations, 3 features\n",
    "    spatial_shape=(64, 64, 2),  # 64x64 grid, 2 channels\n",
    "    temporal_window=72\n",
    ")\n",
    "\n",
    "# Compile model\n",
    "print(\"\\nCompiling model...\")\n",
    "model.compile_model()\n",
    "\n",
    "print(\"\\nâœ“ Model ready for training\")\n",
    "print(\"\\nModel Summary:\")\n",
    "print(model.get_model_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb97dd34",
   "metadata": {},
   "source": [
    "## ðŸš€ Step 8: Train the Model\n",
    "\n",
    "This will take several minutes depending on the number of epochs and GPU availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83fde8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import ModelTrainer\n",
    "import time\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = ModelTrainer(config, model, preprocessor)\n",
    "\n",
    "# Prepare training data\n",
    "print(\"Loading training data...\")\n",
    "X_train, y_train, X_val, y_val = trainer.prepare_training_data('data/raw')\n",
    "\n",
    "print(f\"\\nTraining samples: {len(X_train[0])}\")\n",
    "print(f\"Validation samples: {len(X_val[0])}\")\n",
    "\n",
    "# Start training\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "history = trainer.train(\n",
    "    X_train, y_train, \n",
    "    X_val, y_val,\n",
    "    checkpoint_dir='models/checkpoints'\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"âœ“ TRAINING COMPLETED in {training_time/60:.2f} minutes\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baa9391",
   "metadata": {},
   "source": [
    "## ðŸ“Š Step 9: Visualize Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cc11e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training history\n",
    "trainer.plot_training_history()\n",
    "\n",
    "# Display final metrics\n",
    "print(\"\\nFinal Training Metrics:\")\n",
    "print(f\"  Loss: {history.history['loss'][-1]:.4f}\")\n",
    "print(f\"  Val Loss: {history.history['val_loss'][-1]:.4f}\")\n",
    "\n",
    "if 'risk_probability_accuracy' in history.history:\n",
    "    print(f\"  Risk Accuracy: {history.history['risk_probability_accuracy'][-1]:.4f}\")\n",
    "    print(f\"  Val Risk Accuracy: {history.history['val_risk_probability_accuracy'][-1]:.4f}\")\n",
    "\n",
    "if 'risk_probability_auc' in history.history:\n",
    "    print(f\"  AUC: {history.history['risk_probability_auc'][-1]:.4f}\")\n",
    "    print(f\"  Val AUC: {history.history['val_risk_probability_auc'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cddbef",
   "metadata": {},
   "source": [
    "## ðŸ’¾ Step 10: Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd738eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Save model to local Colab storage\n",
    "model_path = 'models/best_model.keras'\n",
    "model.save_model(model_path)\n",
    "print(f\"âœ“ Model saved to: {model_path}\")\n",
    "\n",
    "# Save preprocessor scalers\n",
    "scaler_dir = 'models/scalers'\n",
    "os.makedirs(scaler_dir, exist_ok=True)\n",
    "preprocessor.save_scalers(scaler_dir)\n",
    "print(f\"âœ“ Scalers saved to: {scaler_dir}\")\n",
    "\n",
    "# Copy to Google Drive (if mounted)\n",
    "try:\n",
    "    import shutil\n",
    "    drive_model_path = '/content/drive/MyDrive/tsunami_warning_system/models/best_model.keras'\n",
    "    drive_scaler_dir = '/content/drive/MyDrive/tsunami_warning_system/models/scalers'\n",
    "    \n",
    "    shutil.copy(model_path, drive_model_path)\n",
    "    os.makedirs(drive_scaler_dir, exist_ok=True)\n",
    "    shutil.copytree(scaler_dir, drive_scaler_dir, dirs_exist_ok=True)\n",
    "    \n",
    "    print(f\"\\nâœ“ Model also saved to Google Drive:\")\n",
    "    print(f\"  {drive_model_path}\")\n",
    "    print(f\"  {drive_scaler_dir}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nNote: Could not save to Google Drive (not mounted or error): {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17f8046",
   "metadata": {},
   "source": [
    "## â¬‡ï¸ Step 11: Download Model Files\n",
    "\n",
    "Download the trained model to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a47fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import zipfile\n",
    "\n",
    "# Create a zip file with model and scalers\n",
    "zip_path = 'tsunami_model.zip'\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
    "    # Add model file\n",
    "    zipf.write('models/best_model.keras', 'best_model.keras')\n",
    "    \n",
    "    # Add scaler files\n",
    "    for root, dirs, file_list in os.walk('models/scalers'):\n",
    "        for file in file_list:\n",
    "            file_path = os.path.join(root, file)\n",
    "            arcname = os.path.join('scalers', file)\n",
    "            zipf.write(file_path, arcname)\n",
    "\n",
    "print(\"âœ“ Created zip file with model and scalers\")\n",
    "\n",
    "# Download the zip file\n",
    "print(\"\\nDownloading... (check your browser downloads)\")\n",
    "files.download(zip_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7eada0",
   "metadata": {},
   "source": [
    "## ðŸ§ª Step 12: Test the Trained Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3f655b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create test data\n",
    "print(\"Testing model with synthetic data...\\n\")\n",
    "\n",
    "test_eq = np.random.randn(1, 10, 4)\n",
    "test_ocean = np.random.randn(1, 5, 3)\n",
    "test_spatial = np.random.randn(1, 64, 64, 2)\n",
    "\n",
    "# Make prediction\n",
    "risk_prob, confidence, risk_class = model.predict(\n",
    "    test_eq, test_ocean, test_spatial\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"Test Prediction Results:\")\n",
    "print(f\"  Risk Probability: {risk_prob[0][0]:.3f}\")\n",
    "print(f\"  Confidence: {confidence[0][0]:.3f}\")\n",
    "print(f\"  Risk Class Probabilities: {risk_class[0]}\")\n",
    "\n",
    "class_names = ['None', 'Low', 'Medium', 'High']\n",
    "predicted_class = np.argmax(risk_class[0])\n",
    "print(f\"  Predicted Risk Level: {class_names[predicted_class]}\")\n",
    "\n",
    "print(\"\\nâœ“ Model is working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f0b609",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Next Steps\n",
    "\n",
    "### After Training:\n",
    "\n",
    "1. **Download the model** (done in Step 11)\n",
    "\n",
    "2. **Deploy to your server:**\n",
    "   ```bash\n",
    "   # Extract the downloaded zip file\n",
    "   unzip tsunami_model.zip\n",
    "   \n",
    "   # Copy files to your project\n",
    "   cp best_model.keras models/\n",
    "   cp -r scalers models/\n",
    "   ```\n",
    "\n",
    "3. **Run the application:**\n",
    "   ```bash\n",
    "   python main.py\n",
    "   ```\n",
    "\n",
    "4. **Or test monitoring:**\n",
    "   ```bash\n",
    "   python monitor.py --once\n",
    "   ```\n",
    "\n",
    "### Training Tips:\n",
    "\n",
    "- **More epochs**: Set `epochs = 100` or more for better accuracy\n",
    "- **Real data**: Replace synthetic data with actual NOAA tsunami database\n",
    "- **GPU runtime**: Use GPU for faster training (Runtime â†’ Change runtime type â†’ GPU)\n",
    "- **Save checkpoints**: Best model is automatically saved during training\n",
    "- **Monitor training**: Watch the loss curves to avoid overfitting\n",
    "\n",
    "### Model Performance:\n",
    "\n",
    "Expected performance with sufficient training:\n",
    "- Validation accuracy: > 90%\n",
    "- AUC score: > 0.85\n",
    "- Inference time: < 2 seconds\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸŒŠ Your tsunami prediction model is now trained and ready to use!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
